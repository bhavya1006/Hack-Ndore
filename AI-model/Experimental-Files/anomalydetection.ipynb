{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.49612403100775193\n",
      "Validation Accuracy: 0.935\n",
      "Validation Precision: 0.3950617283950617\n",
      "Validation Recall: 0.6666666666666666\n",
      "\n",
      "Classification Report on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       952\n",
      "           1       0.96      1.00      0.98        48\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       0.98      1.00      0.99      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "Test F1 Score: 0.8376068376068375\n",
      "Test Accuracy: 0.981\n",
      "Test Precision: 0.7205882352941176\n",
      "Test Recall: 1.0\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       951\n",
      "           1       0.72      1.00      0.84        49\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.86      0.99      0.91      1000\n",
      "weighted avg       0.99      0.98      0.98      1000\n",
      "\n",
      "Test F1 Score: 0.8376068376068375\n",
      "Test Accuracy: 0.981\n",
      "Test Precision: 0.7205882352941176\n",
      "Test Recall: 1.0\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       951\n",
      "           1       0.72      1.00      0.84        49\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.86      0.99      0.91      1000\n",
      "weighted avg       0.99      0.98      0.98      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_df = pd.read_csv('..\\dataset\\indore_water_supply_train.csv')\n",
    "validation_df = pd.read_csv('..\\dataset\\indore_water_supply_validation.csv')\n",
    "test_df = pd.read_csv('..\\dataset\\indore_water_supply_test.csv')\n",
    "train_df['Consumption_per_Person'] = train_df['Water Consumption (liters)'] / train_df['Household Size']\n",
    "validation_df['Consumption_per_Person'] = validation_df['Water Consumption (liters)'] / validation_df['Household Size']\n",
    "test_df['Consumption_per_Person'] = test_df['Water Consumption (liters)'] / test_df['Household Size']\n",
    "features = [\"Water Consumption (liters)\", \"Household Size\", \"Pressure (bar)\", \"Temperature (C)\",'Consumption_per_Person']\n",
    "\n",
    "X_train = train_df[features]\n",
    "X_val = validation_df[features]\n",
    "y_val = validation_df[\"Anomaly\"]\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[\"Anomaly\"]\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "iso_forest = IsolationForest(contamination=0.07, random_state=42)\n",
    "iso_forest.fit(X_train_scaled)\n",
    "val_predictions = iso_forest.predict(X_val_scaled)\n",
    "val_predictions = [1 if pred == -1 else 0 for pred in val_predictions]  # Convert -1 to 1 and 1 to 0\n",
    "\n",
    "test_predictions = iso_forest.predict(X_test_scaled)\n",
    "test_predictions = [1 if pred == -1 else 0 for pred in test_predictions]  # Convert -1 to 1 and 1 to 0\n",
    "val_scores = iso_forest.decision_function(X_val_scaled)\n",
    "scaler = MinMaxScaler()\n",
    "val_scores_scaled = scaler.fit_transform(val_scores.reshape(-1, 1))\n",
    "threshold = 0.3\n",
    "val_predictions = (val_scores_scaled < threshold).astype(int)\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "lof_predictions = lof.fit_predict(X_val_scaled)\n",
    "lof_predictions = [1 if pred == -1 else 0 for pred in lof_predictions]\n",
    "\n",
    "svm = OneClassSVM(nu=0.1, kernel='rbf', gamma=0.1)\n",
    "svm.fit(X_train_scaled)\n",
    "svm_predictions = svm.predict(X_val_scaled)\n",
    "svm_predictions = [1 if pred == -1 else 0 for pred in svm_predictions]\n",
    "\n",
    "# Ensemble voting (majority vote)\n",
    "ensemble_predictions = [1 if (iso_pred + lof_pred + svm_pred) >= 2 else 0 \n",
    "                        for iso_pred, lof_pred, svm_pred in zip(val_predictions, lof_predictions, svm_predictions)]\n",
    "\n",
    "f1_val = f1_score(y_val, ensemble_predictions)\n",
    "accuracy_val = accuracy_score(y_val, ensemble_predictions)\n",
    "precision_val = precision_score(y_val, ensemble_predictions)\n",
    "recall_val = recall_score(y_val, ensemble_predictions)\n",
    "\n",
    "print(f\"Validation F1 Score: {f1_val}\")\n",
    "print(f\"Validation Accuracy: {accuracy_val}\")\n",
    "print(f\"Validation Precision: {precision_val}\")\n",
    "print(f\"Validation Recall: {recall_val}\")\n",
    "\n",
    "print(\"\\nClassification Report on Validation Set:\")\n",
    "print(classification_report(y_val, val_predictions))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "f1_test = f1_score(y_test, test_predictions)\n",
    "accuracy_test = accuracy_score(y_test, test_predictions)\n",
    "precision_test = precision_score(y_test, test_predictions)\n",
    "recall_test = recall_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"Test F1 Score: {f1_test}\")\n",
    "print(f\"Test Accuracy: {accuracy_test}\")\n",
    "print(f\"Test Precision: {precision_test}\")\n",
    "print(f\"Test Recall: {recall_test}\")\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, test_predictions))\n",
    "# Evaluate the model on test set\n",
    "f1_test = f1_score(y_test, test_predictions)\n",
    "accuracy_test = accuracy_score(y_test, test_predictions)\n",
    "precision_test = precision_score(y_test, test_predictions)\n",
    "recall_test = recall_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"Test F1 Score: {f1_test}\")\n",
    "print(f\"Test Accuracy: {accuracy_test}\")\n",
    "print(f\"Test Precision: {precision_test}\")\n",
    "print(f\"Test Recall: {recall_test}\")\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, test_predictions))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
